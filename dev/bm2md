#!/usr/bin/python3

import argparse
import itertools
import json
import logging
import platform
from pathlib import Path
import sys

from bmutils import Benchmarks, axes, build
from mdutils.mdutils import MdUtils
from mdutils.tools.TextUtils import TextUtils

other_axes = axes.copy()
x_axis = other_axes.pop("dispatch")

parser = argparse.ArgumentParser()
parser.add_argument("benchmarks", nargs="?", type=Path, default=Path("build"))
parser.add_argument("--prefix", "-p", default="benchmarks")
parser.add_argument("--objects", "-O", type=int)
parser.add_argument("--hierarchies", "-H", type=int)
parser.add_argument("--compare", "-c")
parser.add_argument("--log-level")
args, other_args = parser.parse_known_args()

if args.log_level:
    logging.basicConfig(level=getattr(logging, args.log_level.upper()))

if Path(args.prefix).suffix == ".json":
    print("reading results from", args.prefix)
    with open(args.prefix) as fh:
        results = Benchmarks.parse(json.load(fh))
    prefix = Path(args.prefix).stem
else:
    if args.hierarchies is not None:
        build(args.benchmarks, args.hierarchies)
    exe = args.benchmarks.joinpath("tests", "benchmarks")
    print(f"running {exe}")
    results = Benchmarks.run(exe, *other_args, objects=args.objects)
    prefix = f"{args.prefix}.{platform.node()}.{results.context.yomm2.compiler}"

    with open(f"{prefix}.json", "w") as fh:
        print("writing results to", f"{prefix}.json")
        json.dump(results.data, fh, indent=4)

if args.compare is None:
    reference = None
else:
    reference_file = (
        f"{args.compare}.{platform.node()}.{results.context.yomm2.compiler}.json"
    )
    try:
        with open(reference_file) as fh:
            reference = Benchmarks.parse(json.load(fh))
    except FileNotFoundError:
        sys.exit(f"no reference file {reference_file}")


def display_cv(cv):
    display_cv = f"max cv = {cv * 100:5.1f}%"
    if cv > 0.05:
        display_cv = TextUtils.bold(f"WARNING: {display_cv}")
    md.new_line(display_cv)

# =============================================================

md = MdUtils(file_name=f"{prefix}.md")
print("writing results to", f"{prefix}.md")
results.context.write_md(md)
md.new_line(f"command line: {' '.join(sys.argv)}")
display_cv(max((bm.cv for bm in results.all)))

for other_tags in itertools.product(*other_axes.values()):
    md.new_header(
        level=2,
        title=", ".join([str(tag).replace("_", " ") for tag in other_tags]),
        add_table_of_contents=False,
    )
    table = ["dispatch", "avg", "ratio"]
    if reference:
        table.append("/ ref")
    columns = len(table)
    max_cv = results.get("virtual_function", *other_tags).cv
    for dispatch in x_axis:
        result = results.get(dispatch, *other_tags)
        max_cv = max(max_cv, result.cv)
        row = [
            result.dispatch.replace("_", " "),
            f"{result.mean:6.1f}",
            "1.00" if result.base is None else f"{result.mean / result.base.mean:5.2f}",
        ]
        if reference:
            ref = reference.get(dispatch, *other_tags)
            row.append(
                "{:5.2f}".format(
                    1
                    if ref.base is None
                    else (result.mean / result.base.mean) / (ref.mean / ref.base.mean)
                )
            )
        table.extend(row)

    md.new_table(
        columns=columns,
        rows=int(len(table) / columns),
        text=table,
        text_align="right",
    )
    display_cv(max_cv)


md.create_md_file()

# --benchmark_enable_random_interleaving=true --benchmark_repetitions=5  --benchmark_min_time=2 --benchmark_report_aggregates_only=true
